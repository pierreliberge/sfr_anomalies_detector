import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path

# Import du chargeur de donnÃ©es
from utils.data_loader import DNSDataLoader

def show():
    st.title("ðŸ¤– ModÃ©lisation des Anomalies")
    
    # Initialisation du chargeur de donnÃ©es
    data_loader = DNSDataLoader()
    
    # Fonction pour prÃ©parer les donnÃ©es DNS
    def prepare_dns_data(df):
        # CrÃ©ation des features pour DNS
        df_agg = df.copy()
        df_agg["ratio_test_clients"] = df_agg["nb_test_dns"] / df_agg["nb_client_total"]
        df_agg["std_over_avg_dns"] = df_agg["std_dns_time"] / df_agg["avg_dns_time"]
        df_agg["avg_dns_per_client"] = df_agg["avg_dns_time"] / df_agg["nb_client_total"]
        df_agg["avg_dns_per_test_dns"] = df_agg["avg_dns_time"] / df_agg["nb_test_dns"]
        
        # SÃ©lection des features pour le modÃ¨le
        features = [
            "avg_dns_time", "std_dns_time", "ratio_test_clients", 
            "std_over_avg_dns", "avg_dns_per_client", "avg_dns_per_test_dns", 
            "nb_test_dns", "nb_client_total"
        ]
        
        # Nettoyage des valeurs infinies ou NaN
        df_agg = df_agg.replace([np.inf, -np.inf], np.nan)
        df_agg = df_agg.fillna(df_agg.mean(numeric_only=True))
        
        return df_agg, features
    
    # Fonction pour prÃ©parer les donnÃ©es Scoring
    def prepare_scoring_data(df):
        # CrÃ©ation des features pour Scoring
        df_agg = df.copy()
        df_agg["ratio_test_clients"] = df_agg["nb_test_scoring"] / df_agg["nb_client_total"]
        df_agg["score_per_test"] = df_agg["avg_score_scoring"] / df_agg["nb_test_scoring"]
        df_agg["latence_per_test"] = df_agg["avg_latence_scoring"] / df_agg["nb_test_scoring"]
        
        # SÃ©lection des features pour le modÃ¨le
        features = [
            "avg_score_scoring", "ratio_test_clients", "latence_per_test", 
            "score_per_test", "nb_test_scoring", "nb_client_total"
        ]
        
        # Nettoyage des valeurs infinies ou NaN
        df_agg = df_agg.replace([np.inf, -np.inf], np.nan)
        df_agg = df_agg.fillna(df_agg.mean(numeric_only=True))
        
        return df_agg, features
    
    # Fonction pour application du modÃ¨le Isolation Forest
    def apply_isolation_forest(df, features, contamination):
        X = df[features]
        
        # Gestion des valeurs NaN
        X = X.fillna(X.mean())
        
        # Application du modÃ¨le
        clf = IsolationForest(contamination=contamination, random_state=42)
        df['anomaly'] = clf.fit_predict(X)
        df['anomaly_score'] = clf.decision_function(X)
        
        # Conversion des prÃ©dictions (-1: anomalie, 1: normal) Ã  (1: anomalie, 0: normal)
        df['anomaly'] = df['anomaly'].map({1: 0, -1: 1})
        
        return df
    
    # Fonction pour afficher les statistiques sur les anomalies
    def display_anomaly_stats(df, title, model_type):
        anomalies = df[df['anomaly'] == 1]
        
        st.subheader(f"Statistiques des anomalies - {title}")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Nombre total d'anomalies", len(anomalies))
        with col2:
            # Calcul du nombre de jours
            unique_dates = df['date_hour'].dt.date.unique()
            nb_days = len(unique_dates)
            avg_anomalies_per_day = len(anomalies) / nb_days if nb_days > 0 else 0
            st.metric("Anomalies moyennes par jour", f"{avg_anomalies_per_day:.2f}")
        with col3:
            anomaly_rate = len(anomalies) / len(df) * 100
            st.metric("Taux d'anomalies", f"{anomaly_rate:.2f}%")
            
        # Distribution temporelle des anomalies
        anomalies['date'] = anomalies['date_hour'].dt.date
        anomalies_by_date = anomalies.groupby('date').size().reset_index(name='count')
        
        if not anomalies_by_date.empty:
            fig = px.line(anomalies_by_date, x='date', y='count', 
                          title=f"Distribution temporelle des anomalies - {title}",
                          labels={'count': "Nombre d'anomalies", 'date': 'Date'})
            st.plotly_chart(fig, use_container_width=True)
        
        # Top des caractÃ©ristiques anomaliques
        if not anomalies.empty:
            st.subheader("CaractÃ©ristiques des anomalies")
            # RÃ©cupÃ©ration de l'identifiant principal
            id_col = 'peag_nro' if 'peag_nro' in anomalies.columns else 'olt_name'
            
            # Top des entitÃ©s avec le plus d'anomalies
            top_entities = anomalies.groupby(id_col).size().sort_values(ascending=False).reset_index(name='count')
            if not top_entities.empty:
                st.write(f"Top des {id_col} avec le plus d'anomalies (cliquez sur une ligne pour plus de dÃ©tails):")
                
                # Stocker les donnÃ©es pour l'interaction
                st.session_state[f"{model_type}_top_entities"] = top_entities
                st.session_state[f"{model_type}_anomalies"] = anomalies
                st.session_state[f"{model_type}_all_data"] = df
                st.session_state[f"{model_type}_id_col"] = id_col
                
                # Tableau interactif avec gestion des clics
                st.dataframe(
                    top_entities.head(10), 
                    use_container_width=True,
                    column_config={
                        "count": st.column_config.NumberColumn(
                            "Nombre d'anomalies",
                            help="Nombre total d'anomalies dÃ©tectÃ©es"
                        )
                    },
                    hide_index=True
                )

                if not top_entities.empty:
                    id_col = st.session_state[f"{model_type}_id_col"]
                    selected_entity = st.selectbox(
                        f"SÃ©lectionnez un {id_col} pour voir les dÃ©tails:",
                        options=top_entities[id_col].tolist(),
                        key=f"{model_type}_entity_selector"
                    )
                    st.session_state[f"{model_type}_selected_entity"] = selected_entity
                
                # Afficher le graphique pour l'entitÃ© sÃ©lectionnÃ©e (si disponible)
                selected_entity = st.session_state.get(f"{model_type}_selected_entity", None)
                
                if selected_entity:
                    display_entity_details(
                        df=df, 
                        entity_name=selected_entity, 
                        id_col=id_col, 
                        model_type=model_type
                    )
    
    # Fonction pour afficher les dÃ©tails d'une entitÃ© spÃ©cifique
    def display_entity_details(df, entity_name, id_col, model_type):
        entity_data = df[df[id_col] == entity_name]
        
        st.markdown(f"### DÃ©tails pour {id_col}: **{entity_name}**")
        
        # Compteur d'anomalies pour cette entitÃ©
        anomaly_count = entity_data['anomaly'].sum()
        total_count = len(entity_data)
        anomaly_rate = (anomaly_count / total_count * 100) if total_count > 0 else 0
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Nombre d'anomalies", anomaly_count)
        with col2:
            st.metric("Taux d'anomalies", f"{anomaly_rate:.2f}%")
        
        # Graphique avec la sÃ©rie temporelle pour cette entitÃ©
        y_metric = 'avg_dns_time' if model_type == 'dns' else 'avg_score_scoring'
        title_metric = 'Temps DNS moyen' if model_type == 'dns' else 'Score moyen'
        
        fig = px.scatter(entity_data, x='date_hour', y=y_metric, 
                         color='anomaly', color_discrete_map={0: 'blue', 1: 'red'},
                         title=f'{title_metric} pour {entity_name} (rouge = anomalie)')
        
        fig.update_layout(legend_title_text='Anomalie',
                         xaxis_title="Date",
                         yaxis_title=title_metric)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Statistiques descriptives
        st.markdown("#### Statistiques descriptives")
        st.dataframe(entity_data.describe().T)
    
    # Chargement des donnÃ©es brutes
    df_raw = data_loader.load_data()
    
    # Interface utilisateur
    st.subheader("ParamÃ¨tres de modÃ©lisation")
    
    # SÃ©lection de la pÃ©riode
    min_date = df_raw['date_hour'].min().date()
    max_date = df_raw['date_hour'].max().date()
    
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("Date de dÃ©but", 
                                   value=min_date,
                                   min_value=min_date, 
                                   max_value=max_date)
    with col2:
        end_date = st.date_input("Date de fin", 
                                 value=max_date,
                                 min_value=min_date, 
                                 max_value=max_date)
    
    # SÃ©lection du seuil de contamination
    contamination = st.slider("Seuil de contamination (% attendu d'anomalies)", 
                              min_value=0.01, max_value=0.3, value=0.05, step=0.01,
                              format="%.2f",
                              help="Plus la valeur est Ã©levÃ©e, plus le modÃ¨le dÃ©tectera d'anomalies")
    
    # Chargement des donnÃ©es agrÃ©gÃ©es selon le niveau choisi
    # On charge les donnÃ©es aprÃ¨s la sÃ©lection pour appliquer directement le filtre de date
    @st.cache_data
    def load_filtered_data(aggregation_level, start_date, end_date):
        df = data_loader.load_aggregated_data(aggregation_level)
        df_filtered = df[(df['date_hour'].dt.date >= start_date) & 
                         (df['date_hour'].dt.date <= end_date)]
        return df_filtered
    
    # CrÃ©ation des onglets
    tabs = st.tabs(["ðŸŒ ModÃ¨le DNS", "ðŸ“Š ModÃ¨le Scoring", "ðŸ”„ Comparaison"])
    
    with tabs[0]:  # Tab DNS
        st.header("DÃ©tection d'anomalies sur les donnÃ©es DNS")
        
        df_dns = load_filtered_data(st.session_state.aggregation_level, start_date, end_date)
        
        # VÃ©rifier si des donnÃ©es DNS existent
        if 'nb_test_dns' not in df_dns.columns or df_dns['nb_test_dns'].sum() == 0:
            st.warning("Pas de donnÃ©es DNS disponibles pour la pÃ©riode et le niveau d'agrÃ©gation sÃ©lectionnÃ©s.")
        else:
            df_dns = df_dns[df_dns['nb_test_dns'] > 0]
            
            # PrÃ©paration des donnÃ©es
            df_dns_prepared, features_dns = prepare_dns_data(df_dns)
            
            st.write("**Features utilisÃ©es pour le modÃ¨le DNS :**")
            st.write(", ".join(features_dns))
            
            # Application du modÃ¨le
            if st.button("ðŸš€ Lancer la dÃ©tection d'anomalies DNS", key="dns_button"):
                with st.spinner("DÃ©tection des anomalies en cours..."):
                    df_dns_results = apply_isolation_forest(df_dns_prepared, features_dns, contamination)
                    
                    # Affichage des rÃ©sultats
                    fig = px.scatter(df_dns_results, x='date_hour', y='avg_dns_time', 
                                     color='anomaly', color_discrete_map={0: 'blue', 1: 'red'},
                                     hover_data=['peag_nro' if 'peag_nro' in df_dns_results.columns else None, 
                                                'olt_name' if 'olt_name' in df_dns_results.columns else None,
                                                'anomaly_score'],
                                     title='DÃ©tection des anomalies DNS - Temps DNS moyen')
                    
                    fig.update_layout(legend_title_text='Anomalie',
                                     xaxis_title="Date",
                                     yaxis_title="Temps DNS moyen (ms)")
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Statistiques des anomalies
                    display_anomaly_stats(df_dns_results, "DNS", "dns")
                    
                    # Sauvegarde des rÃ©sultats dans la session
                    st.session_state.dns_results = df_dns_results
    
    with tabs[1]:  # Tab Scoring
        st.header("DÃ©tection d'anomalies sur les donnÃ©es de Scoring")
        
        df_scoring = load_filtered_data(st.session_state.aggregation_level, start_date, end_date)
        
        # VÃ©rifier si des donnÃ©es Scoring existent
        if 'nb_test_scoring' not in df_scoring.columns or df_scoring['nb_test_scoring'].sum() == 0:
            st.warning("Pas de donnÃ©es Scoring disponibles pour la pÃ©riode et le niveau d'agrÃ©gation sÃ©lectionnÃ©s.")
        else:
            df_scoring = df_scoring[df_scoring['nb_test_scoring'] > 0]
            
            # PrÃ©paration des donnÃ©es
            df_scoring_prepared, features_scoring = prepare_scoring_data(df_scoring)
            
            st.write("**Features utilisÃ©es pour le modÃ¨le Scoring :**")
            st.write(", ".join(features_scoring))
            
            # Application du modÃ¨le
            if st.button("ðŸš€ Lancer la dÃ©tection d'anomalies Scoring", key="scoring_button"):
                with st.spinner("DÃ©tection des anomalies en cours..."):
                    df_scoring_results = apply_isolation_forest(df_scoring_prepared, features_scoring, contamination)
                    
                    # Affichage des rÃ©sultats
                    fig = px.scatter(df_scoring_results, x='date_hour', y='avg_score_scoring', 
                                     color='anomaly', color_discrete_map={0: 'blue', 1: 'red'},
                                     hover_data=['peag_nro' if 'peag_nro' in df_scoring_results.columns else None, 
                                                'olt_name' if 'olt_name' in df_scoring_results.columns else None,
                                                'anomaly_score'],
                                     title='DÃ©tection des anomalies Scoring - Score moyen')
                    
                    fig.update_layout(legend_title_text='Anomalie',
                                     xaxis_title="Date",
                                     yaxis_title="Score moyen")
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Statistiques des anomalies
                    display_anomaly_stats(df_scoring_results, "Scoring", "scoring")
                    
                    # Sauvegarde des rÃ©sultats dans la session
                    st.session_state.scoring_results = df_scoring_results
    
    with tabs[2]:  # Tab Comparaison
        st.header("Comparaison des anomalies DNS et Scoring")
        
        if 'dns_results' not in st.session_state or 'scoring_results' not in st.session_state:
            st.info("Veuillez d'abord lancer les modÃ¨les DNS et Scoring pour voir la comparaison.")
        else:
            # Fusionner les rÃ©sultats sur la base des colonnes communes
            merge_cols = ['date_hour']
            if st.session_state.aggregation_level == "peag_nro":
                merge_cols.append('peag_nro')
            elif st.session_state.aggregation_level == "olt_name":
                merge_cols.append('olt_name')
            elif st.session_state.aggregation_level == "peag_nro & olt_name":
                merge_cols.extend(['peag_nro', 'olt_name'])
            
            df_combined = pd.merge(
                st.session_state.dns_results[merge_cols + ['anomaly']].rename(columns={'anomaly': 'dns_anomaly'}),
                st.session_state.scoring_results[merge_cols + ['anomaly']].rename(columns={'anomaly': 'scoring_anomaly'}),
                on=merge_cols
            )
            
            # CrÃ©er une catÃ©gorie combinÃ©e
            df_combined['combined'] = 'Normal'
            df_combined.loc[df_combined['dns_anomaly'] == 1, 'combined'] = 'DNS Anomalie'
            df_combined.loc[df_combined['scoring_anomaly'] == 1, 'combined'] = 'Scoring Anomalie'
            df_combined.loc[(df_combined['dns_anomaly'] == 1) & (df_combined['scoring_anomaly'] == 1), 'combined'] = 'Double Anomalie'
            
            # Visualisation
            fig = px.scatter(df_combined, x='date_hour', y=range(len(df_combined)), 
                             color='combined', 
                             color_discrete_map={
                                 'Normal': 'blue',
                                 'DNS Anomalie': 'orange',
                                 'Scoring Anomalie': 'green',
                                 'Double Anomalie': 'red'
                             },
                             hover_data=merge_cols,
                             title='Comparaison des anomalies DNS et Scoring')
            
            fig.update_layout(legend_title_text='Type',
                             xaxis_title="Date",
                             yaxis_title="Index")
            fig.update_yaxes(showticklabels=False)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Statistiques de comparaison
            st.subheader("Statistiques de comparaison")
            
            # Metrics dans des colonnes
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                normal_count = len(df_combined[df_combined['combined'] == 'Normal'])
                st.metric("Points normaux", normal_count)
            with col2:
                dns_only = len(df_combined[df_combined['combined'] == 'DNS Anomalie'])
                st.metric("Anomalies DNS uniquement", dns_only)
            with col3:
                scoring_only = len(df_combined[df_combined['combined'] == 'Scoring Anomalie'])
                st.metric("Anomalies Scoring uniquement", scoring_only)
            with col4:
                both = len(df_combined[df_combined['combined'] == 'Double Anomalie'])
                st.metric("Doubles anomalies", both)
            
            # Matrice de confusion
            confusion_matrix = pd.crosstab(df_combined['dns_anomaly'], df_combined['scoring_anomaly'], 
                                           rownames=['DNS'], colnames=['Scoring'])
            
            st.subheader("Matrice de concordance des anomalies")
            st.dataframe(confusion_matrix)
            
            # Taux de concordance
            total = len(df_combined)
            concordance = (normal_count + both) / total * 100
            st.metric("Taux de concordance DNS/Scoring", f"{concordance:.2f}%")
            
            # Liste des anomalies
            st.subheader("Liste des anomalies dÃ©tectÃ©es")
            anomalies_df = df_combined[df_combined['combined'] != 'Normal'].copy()
            anomalies_df['date'] = anomalies_df['date_hour'].dt.date
            anomalies_df['hour'] = anomalies_df['date_hour'].dt.hour
            st.dataframe(anomalies_df)
            
            # Option de tÃ©lÃ©chargement
            csv = anomalies_df.to_csv(index=False)
            st.download_button(
                label="ðŸ“¥ TÃ©lÃ©charger les rÃ©sultats des anomalies",
                data=csv,
                file_name=f"anomalies_{start_date}_{end_date}.csv",
                mime="text/csv",
            )
            
            # Top des entitÃ©s avec des doubles anomalies
            if both > 0:
                st.subheader("EntitÃ©s avec des doubles anomalies")
                id_col = merge_cols[1] if len(merge_cols) > 1 else None
                if id_col:
                    double_anomalies = df_combined[df_combined['combined'] == 'Double Anomalie']
                    top_double = double_anomalies.groupby(id_col).size().sort_values(ascending=False).reset_index(name='count')
                    st.dataframe(top_double.head(10), use_container_width=True)